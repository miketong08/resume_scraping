{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping resumes off Craigslist\n",
    "---\n",
    "\n",
    "To gather summary statistics, you'll probably first want to figure out where the most populated areas are. There is a few ways to go about this, but to make searches easier on Craigslist, zip codes make life a lot easier, and we can define a radius around that zip code. The 100 most populated zip codes was found via http://localistica.com/usa/zipcodes/most-populated-zipcodes/.  \n",
    "\n",
    "## Sections\n",
    "---\n",
    "- [Part 1: What is a WebDriver?](#Part-1:-What-is-a-WebDriver?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Video  # Jupyter notebook methods\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./../scripts/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: What is a Webdriver?\n",
    "---\n",
    "Webdriving in simple terms is __the automation of actions on a web browser__, such as Chrome and FireFox. You can think of this as being able to tell a computer how to navigate websites and perform human actions such as copy, download, etc. automatically. Most, Web Drivers are driven by analyzing HTML, which makes knowledge of the markup language extremely helpful in performing this task. Web driving is an extremely popular form of webscraping, and is also known as a webcrawler. \n",
    "\n",
    "With Python, the most common webdriver can be found in the selenium suite, which is offered in C#, Java, JavaScript, Python, and Ruby. More information on the platform can be found [here](https://www.seleniumhq.org/).\n",
    "\n",
    "Now let's get started! To kick things off, we're going to first have to download our browser's webdriver; we're going to be sticking with Google Chrome for consistency. First, we need to get our browser's specific webdriver applciation, for Google (chromedriver), this can be found at their [chromium website](https://chromedriver.chromium.org/). The version of the driver needs to math your current browser's version. To check for this, watch the video below. \n",
    "\n",
    "Once chromedriver is downloaded, move it into the `./../assets` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"1000\" height=\"600\" controls>\n",
       "  <source src=\"./../assets/videos/webdriver_download.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"1000\" height=\"600\" controls>\n",
    "  <source src=\"./../assets/videos/webdriver_download.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With chromedriver downloaded, let's get to some code! We're going to first import the `webdriver` module from selenium with the line below. Next, we define a variable called `driver` which will open up a chrome browser by calling the `webdriver.Chrome` method (note the path to the webdriver application downloaded from above). This browser however is special and can be controlled via the `driver` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"./../assets/chromedriver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woho! We'e got Chrome open. Now this browser works like your normal one, you can click around and even go to whatever webpage you like. However, the special part of this browser is that we use the `driver`'s methods to perform these tasks for us. Let's make our first automated move to Craigslist! This is done with the `get` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"http://craigslist.org\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet, we're able to jump from page to page... now what? Well, let's see if we can pull some HTML information from this page. \n",
    "One way to extract HTML information is by using the `find_element_by_xpath` method from the driver variable ([more on XPath](https://www.w3schools.com/xml/xpath_intro.asp)). This seeks out the particular web element on the page and is returned as a WebElement class. We'll start with the front page text elements by extracting the `/html` element and assigning it to the variable front_page. \n",
    "\n",
    "With the instatiated variable, if the xpath is found, the variable class contains the attribute `text` which will print a string of all the text found in the associated HTML element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "washington, DC\n",
      "doc\n",
      "nva\n",
      "mld\n",
      "craigslist\n",
      "create a posting\n",
      "my account\n",
      "event calendar\n",
      "M T W T F S S\n",
      "2 3 4 5 6 7 8\n",
      "9 10 11 12 13 14 15\n",
      "16 17 18 19 20 21 22\n",
      "23 24 25 26 27 28 29\n",
      "help, faq, abuse, legal\n",
      "avoid scams & fraud\n",
      "personal safety tips\n",
      "terms of usenew\n",
      "privacy policy\n",
      "system status\n",
      "about craigslist\n",
      "craigslist is hiring in sf\n",
      "craigslist open source\n",
      "craigslist blog\n",
      "best-of-craigslist\n",
      "craigslist TV\n",
      "\"craigslist joe\"\n",
      "craig connects\n",
      "community\n",
      "activities\n",
      "artists\n",
      "childcare\n",
      "classes\n",
      "events\n",
      "general\n",
      "groups\n",
      "local news\n",
      "lost+found\n",
      "missed connections\n",
      "musicians\n",
      "pets\n",
      "politics\n",
      "rants & raves\n",
      "rideshare\n",
      "volunteers\n",
      "services\n",
      "automotive\n",
      "beauty\n",
      "cell/mobile\n",
      "computer\n",
      "creative\n",
      "cycle\n",
      "event\n",
      "farm+garden\n",
      "financial\n",
      "household\n",
      "labor/move\n",
      "legal\n",
      "lessons\n",
      "marine\n",
      "pet\n",
      "real estate\n",
      "skilled trade\n",
      "sm biz ads\n",
      "travel/vac\n",
      "write/ed/tran\n",
      "discussion forums\n",
      "android\n",
      "apple\n",
      "arts\n",
      "atheist\n",
      "autos\n",
      "beauty\n",
      "bikes\n",
      "celebs\n",
      "comp\n",
      "cosmos\n",
      "diet\n",
      "divorce\n",
      "dying\n",
      "eco\n",
      "feedbk\n",
      "film\n",
      "fixit\n",
      "food\n",
      "frugal\n",
      "gaming\n",
      "garden\n",
      "haiku\n",
      "help\n",
      "history\n",
      "housing\n",
      "jobs\n",
      "jokes\n",
      "legal\n",
      "linux\n",
      "manners\n",
      "marriage\n",
      "money\n",
      "music\n",
      "open\n",
      "outdoor\n",
      "parent\n",
      "pets\n",
      "philos\n",
      "photo\n",
      "politics\n",
      "psych\n",
      "recover\n",
      "religion\n",
      "rofo\n",
      "science\n",
      "spirit\n",
      "sports\n",
      "super\n",
      "tax\n",
      "travel\n",
      "tv\n",
      "vegan\n",
      "words\n",
      "writing\n",
      "housing\n",
      "apts / housing\n",
      "housing swap\n",
      "housing wanted\n",
      "office / commercial\n",
      "parking / storage\n",
      "real estate for sale\n",
      "rooms / shared\n",
      "rooms wanted\n",
      "sublets / temporary\n",
      "vacation rentals\n",
      "for sale\n",
      "antiques\n",
      "appliances\n",
      "arts+crafts\n",
      "atv/utv/sno\n",
      "auto parts\n",
      "aviation\n",
      "baby+kid\n",
      "barter\n",
      "beauty+hlth\n",
      "bike parts\n",
      "bikes\n",
      "boat parts\n",
      "boats\n",
      "books\n",
      "business\n",
      "cars+trucks\n",
      "cds/dvd/vhs\n",
      "cell phones\n",
      "clothes+acc\n",
      "collectibles\n",
      "computer parts\n",
      "computers\n",
      "electronics\n",
      "farm+garden\n",
      "free\n",
      "furniture\n",
      "garage sale\n",
      "general\n",
      "heavy equip\n",
      "household\n",
      "jewelry\n",
      "materials\n",
      "motorcycle parts\n",
      "motorcycles\n",
      "music instr\n",
      "photo+video\n",
      "rvs+camp\n",
      "sporting\n",
      "tickets\n",
      "tools\n",
      "toys+games\n",
      "trailers\n",
      "video gaming\n",
      "wanted\n",
      "wheels+tires\n",
      "jobs\n",
      "accounting+finance\n",
      "admin / office\n",
      "arch / engineering\n",
      "art / media / design\n",
      "biotech / science\n",
      "business / mgmt\n",
      "customer service\n",
      "education\n",
      "etc / misc\n",
      "food / bev / hosp\n",
      "general labor\n",
      "government\n",
      "human resources\n",
      "legal / paralegal\n",
      "manufacturing\n",
      "marketing / pr / ad\n",
      "medical / health\n",
      "nonprofit sector\n",
      "real estate\n",
      "retail / wholesale\n",
      "sales / biz dev\n",
      "salon / spa / fitness\n",
      "security\n",
      "skilled trade / craft\n",
      "software / qa / dba\n",
      "systems / network\n",
      "technical support\n",
      "transport\n",
      "tv / film / video\n",
      "web / info design\n",
      "writing / editing\n",
      "gigs\n",
      "computer\n",
      "creative\n",
      "crew\n",
      "domestic\n",
      "event\n",
      "labor\n",
      "talent\n",
      "writing\n",
      "resumes\n",
      "        dansk\n",
      "deutsch\n",
      "english\n",
      "español\n",
      "français\n",
      "italiano\n",
      "português\n",
      "suomi\n",
      "svenska\n",
      "tiếng việt\n",
      "türkçe\n",
      "русский\n",
      "中文\n",
      "日本語\n",
      "한국말\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "nearby cl\n",
      "allentown\n",
      "altoona\n",
      "annapolis\n",
      "baltimore\n",
      "central nj\n",
      "charlottesville\n",
      "cumberland val\n",
      "delaware\n",
      "eastern shore\n",
      "eastern wv\n",
      "frederick\n",
      "fredericksburg\n",
      "harrisburg\n",
      "harrisonburg\n",
      "jersey shore\n",
      "lancaster\n",
      "lynchburg\n",
      "morgantown\n",
      "norfolk\n",
      "philadelphia\n",
      "poconos\n",
      "reading\n",
      "richmond\n",
      "southern md\n",
      "south jersey\n",
      "state college\n",
      "western md\n",
      "williamsport\n",
      "winchester\n",
      "york\n",
      "us cities\n",
      "us states\n",
      "canada\n",
      "cl worldwide\n",
      "\n",
      "© craigslist help safety privacy feedback cl jobs termsnew about mobile\n"
     ]
    }
   ],
   "source": [
    "front_page = driver.find_element_by_xpath('/html')\n",
    "print(front_page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well it unfortunately looks like Craigslist defaults to searching in your local area. You could manually change the location by searching for a new one... or is there an automated way of doing this? \n",
    "\n",
    "Let's see what happens when you search for a new address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"1000\" height=\"600\" autoplay controls>\n",
       "  <source src=\"./../assets/videos/craigslist_hyperlink_zip.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"1000\" height=\"600\" autoplay controls>\n",
    "  <source src=\"./../assets/videos/craigslist_hyperlink_zip.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's subtle, but the hyperlink provides a lot of information. Searching CL in different cities can be done by changing following the format of ```{cityname}.craigslist.org/?search_distance={distance}&postal={zipcode}```. \n",
    "\n",
    "With this knowledge we are now able to change the area that we are searching! So what now? Do we compile a list of every city and their zipcode? Naw, we're smarter than that. There are various approaches to this, such as simply changing the distance to something really large (9999) and pick the middle of America, but a more clever approach may be to find the cities and zipcodes with the highest populations. For learning purposes, let's give the latter a shot!\n",
    "\n",
    "First we're going to find a website that provides a list of the most populated zipcodes, the Google's tells us that http://localistica.com/usa/zipcodes/most-populated-zipcodes/ provides just that information. Let's check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('http://localistica.com/usa/zipcodes/most-populated-zipcodes/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool site! There's also a really convenient table that provides both the zip code and city! But how do we get this table into our Notebook? Yes we can copy and paste, or even worse, manually copy down the values. But here's where selenium shines! We can quickly copy the table into our notebook by extracting the HTML element. This can be done in a few ways but we're going to do this through the `XPath` on the page (this of this as the path to some element, i.e. the table). The video below shows how to get the `XPath` of a webpage element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"1000\" height=\"600\" autoplay controls>\n",
       "  <source src=\"./../assets/videos/getting_xpaths.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"1000\" height=\"600\" autoplay controls>\n",
    "  <source src=\"./../assets/videos/getting_xpaths.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `XPath` copied, we can get the element by telling the driver to `find_element_by_xpath`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage_table = driver.find_element_by_xpath('//*[@id=\"frmAF\"]/div[6]/div/div[1]/div[2]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is returned is a `WebElement` class. It has a few methods and attributes but what we're interested in is the `text` attribute, which contains the raw information. Let's see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "webpage_table type:  <class 'selenium.webdriver.remote.webelement.WebElement'>\n",
      "webpage_table text: \n",
      " ZipCode City Population Growth Age Income per household\n",
      "79936 El Paso TX 115,556 3% 31.00 $42,857.00\n",
      "90011 Los Angeles CA 106,326 2% 26.20 $23,851.00\n",
      "60629 Chicago IL 105,209 -8% 28.80 $40,279.00\n",
      "90650 Norwalk CA 104,765 0% 32.50 $46,012.00\n",
      "90201 Bell Gardens CA 101,479 0% 27.80 $30,029.00\n",
      "77084 Houston TX 101,233 6% 31.20 $53,075.00\n",
      "92335 Fontana CA 99,743 4% 26.90 $35,008.00\n",
      "78521 Brownsville TX 99,632 5% 28.00 $23,426.00\n",
      "77449 Katy TX 99,586 5% 29.60 $59,198.00\n",
      "78572 Mission TX 96,822 22% 31.50 $23,799.00\n",
      "90250 Hawthorne CA 96,593 3% 31.90 $33,656.00\n",
      "90280 South Gate CA 95,430 1% 29.40 $35,744.00\n",
      "11226 Brooklyn NY 94,814 -7% 34.50 $29,498.00\n",
      "90805 Long Beach CA 94,475 1% 29.00 $32,565.00\n",
      "91331 Pacoima CA 93,821 -10% 29.50 $39,225.00\n",
      "08701 Lakewood NJ 93,320 0% 23.90 $35,647.00\n",
      "90044 Los Angeles CA 92,967 3% 28.60 $22,091.00\n",
      "92336 Fontana CA 92,195 4% 30.10 $55,340.00\n",
      "00926 San Juan PR 91,579 -18% 39.10 $26,306.00\n",
      "94565 Pittsburg CA 90,935 6% 31.80 $48,523.00\n",
      "10467 Bronx NY 90,492 -7% 32.80 $29,044.00\n",
      "92683 Westminster CA 89,851 0% 38.70 $49,686.00\n",
      "75052 Grand Prairie TX 89,829 0% 32.30 $60,254.00\n",
      "91342 Sylmar CA 89,725 -2% 31.90 $48,744.00\n",
      "92704 Santa Ana CA 89,527 1% 29.80 $49,923.00\n",
      "30044 Lawrenceville GA 87,610 9% 32.00 $60,427.00\n",
      "10025 New York NY 86,082 -9% 39.50 $49,733.00\n",
      "92503 Riverside CA 85,990 1% 30.80 $44,829.00\n",
      "92804 Anaheim CA 85,970 0% 33.00 $41,887.00\n",
      "78577 Pharr TX 85,926 17% 28.00 $24,216.00\n",
      "75217 Dallas TX 84,944 5% 27.10 $31,532.00\n",
      "92376 Rialto CA 84,462 3% 27.60 $37,568.00\n",
      "93307 Bakersfield CA 84,172 1% 25.60 $26,462.00\n",
      "10456 Bronx NY 84,128 -2% 29.80 $16,664.00\n",
      "10002 New York NY 84,099 3% 39.70 $24,022.00\n",
      "91911 Chula Vista CA 83,259 0% 33.50 $38,010.00\n",
      "91744 La Puente CA 82,850 -2% 30.90 $46,792.00\n",
      "75070 Mckinney TX 82,329 9% 33.90 $84,847.00\n",
      "77036 Houston TX 82,059 12% 29.50 $26,931.00\n",
      "93722 Fresno CA 81,972 6% 29.50 $43,256.00\n",
      "92345 Hesperia CA 81,522 3% 31.20 $41,423.00\n",
      "60618 Chicago IL 81,196 -13% 32.50 $41,355.00\n",
      "93033 Oxnard CA 81,183 -1% 27.80 $46,342.00\n",
      "93550 Palmdale CA 80,635 7% 27.50 $37,484.00\n",
      "95076 Watsonville CA 80,408 -2% 31.10 $45,354.00\n",
      "11230 Brooklyn NY 80,243 -7% 33.90 $32,327.00\n",
      "11368 Corona NY 79,999 -37% 30.80 $34,746.00\n",
      "37013 Antioch TN 79,687 1% 30.80 $46,063.00\n",
      "11373 Elmhurst NY 79,681 -26% 35.30 $38,151.00\n",
      "79912 El Paso TX 79,333 2% 35.00 $48,627.00\n",
      "37211 Nashville TN 79,153 5% 31.20 $37,141.00\n",
      "30043 Lawrenceville GA 78,946 -1% 34.30 $71,424.00\n",
      "11206 Brooklyn NY 78,610 -3% 28.50 $18,661.00\n",
      "10453 Bronx NY 78,009 0% 29.70 $21,109.00\n",
      "92154 San Diego CA 77,614 -2% 32.90 $42,970.00\n",
      "11355 Flushing NY 77,581 -10% 40.50 $36,973.00\n",
      "95823 Sacramento CA 77,463 4% 29.10 $36,001.00\n",
      "77479 Sugar Land TX 77,169 3% 39.00 $96,118.00\n",
      "91706 Baldwin Park CA 76,873 0% 30.50 $41,621.00\n",
      "10458 Bronx NY 76,836 -3% 28.40 $22,072.00\n",
      "92553 Moreno Valley CA 76,811 4% 26.80 $38,554.00\n",
      "90706 Bellflower CA 76,519 0% 31.90 $39,362.00\n",
      "23464 Virginia Beach VA 76,485 5% 36.00 $53,486.00\n",
      "11212 Brooklyn NY 76,440 -10% 31.90 $20,839.00\n",
      "60617 Chicago IL 76,284 -10% 35.40 $35,534.00\n",
      "91709 Chino Hills CA 76,274 1% 36.60 $78,336.00\n",
      "11214 Brooklyn NY 76,206 -16% 38.90 $33,765.00\n",
      "11219 Brooklyn NY 76,158 -21% 27.20 $26,648.00\n",
      "91910 Chula Vista CA 76,052 0% 35.60 $42,970.00\n",
      "22193 Woodbridge VA 76,025 3% 32.60 $67,190.00\n",
      "77429 Cypress TX 76,016 4% 34.80 $78,527.00\n",
      "93535 Lancaster CA 75,841 5% 28.30 $39,747.00\n",
      "66062 Olathe KS 75,688 1% 32.80 $68,682.00\n",
      "93257 Porterville CA 75,659 0% 29.10 $30,995.00\n",
      "30349 Atlanta GA 75,615 10% 31.40 $39,141.00\n",
      "60647 Chicago IL 75,516 -15% 30.70 $35,283.00\n",
      "77584 Pearland TX 75,508 6% 33.60 $70,113.00\n",
      "10452 Bronx NY 75,321 0% 30.20 $20,606.00\n",
      "77573 League City TX 75,290 4% 34.90 $68,458.00\n",
      "11377 Woodside NY 75,274 -19% 35.70 $37,360.00\n",
      "11207 Brooklyn NY 75,149 -24% 31.00 $24,163.00\n",
      "77494 Katy TX 75,051 17% 33.70 $86,488.00\n",
      "75211 Dallas TX 74,937 2% 27.10 $32,702.00\n",
      "11234 Brooklyn NY 74,872 -17% 39.10 $51,446.00\n",
      "28269 Charlotte NC 74,794 5% 32.60 $61,899.00\n",
      "11235 Brooklyn NY 74,770 -5% 45.00 $31,013.00\n",
      "94544 Hayward CA 74,736 2% 32.30 $49,452.00\n",
      "10029 New York NY 74,350 -2% 33.30 $22,232.00\n",
      "60625 Chicago IL 74,239 -5% 32.50 $40,083.00\n",
      "89110 Las Vegas NV 74,141 4% 30.10 $43,073.00\n",
      "92509 Riverside CA 74,036 -1% 29.90 $45,995.00\n",
      "77083 Houston TX 73,953 4% 32.00 $52,931.00\n",
      "91335 Reseda CA 73,881 0% 35.50 $40,792.00\n",
      "85364 Yuma AZ 73,806 -1% 31.10 $31,515.00\n",
      "87121 Albuquerque NM 73,554 -4% 27.90 $34,359.00\n",
      "10468 Bronx NY 73,501 -3% 31.30 $26,852.00\n",
      "90255 Huntington Park CA 73,477 -2% 29.10 $30,375.00\n",
      "93065 Simi Valley CA 73,357 1% 37.10 $72,384.00\n",
      "91710 Chino CA 73,117 -9% 33.00 $55,185.00\n",
      "10462 Bronx NY 73,071 -3% 34.90 $33,735.00\n"
     ]
    }
   ],
   "source": [
    "print(\"webpage_table type: \", type(webpage_table))\n",
    "print(\"webpage_table text: \\n\", webpage_table.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eyy! We got the table! Now we need to get just the zipcodes and cities. This processes is often referred to as _parsing_. Separating the zipcodes and city names is actually not entirely intuitive, nor is it important for this tutorial. We provided a simple helper function that does this from the `ancillary` library as `parse_webpage_table`. Feel free to view that code, or don't and just think of it as a magical black box.\n",
    "\n",
    "The function returns a list of tuples containing ({city}, {zipcode}). Let's see the first few. Note that the spaces in the city names is removed because that's how craigslist processes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ancillary import parse_webpage_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage_table_info = parse_webpage_table(webpage_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ElPaso', '79936'),\n",
       " ('LosAngeles', '90011'),\n",
       " ('Chicago', '60629'),\n",
       " ('Norwalk', '90650'),\n",
       " ('BellGardens', '90201')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webpage_table_info[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! For a sanity check, let's make sure that all 100 rows are captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements parsed table:  100\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of elements parsed table: \", len(webpage_table_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet, now that we're able to search different cities, let's check out resumes! Back to Craigslist!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"http://craigslist.org\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we search a new city and click on ```resumes``` you'll notice that the hyperlink adds a bit more information, in the format of, `https://{city}.craigslist.org/d/resumes/search/rrr?postal={zip}&search_distance={distance}`. This is perfect since we have all the information we need! \n",
    "\n",
    "Let's try this out with one of the cities and zipcodes we found to be heavily populate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "city, zipcode = webpage_table_info[0]\n",
    "distance = 30\n",
    "\n",
    "driver.get(\"https://{}.craigslist.org/d/resumes/search/rrr?postal={}&search_distance={}\".format(city, zipcode, distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We can now start scraping these resume's across all the cities! \n",
    "\n",
    "First we need to get all the hyperlinks associated with the posts, this can be done in the DevTool panel once again (F12). Search through a line highlights the table in the CL page, and copy that `XPath` again. See below for the example, the `XPath` should be `//*[@id=\"sortable-results\"]/ul`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./../assets/images/cl_table_ref.png\" \n",
       "    alt=\"DevTool example for location of CL table element \" \n",
       "    style=\"width:1000px;height:600px;\"\n",
       ">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<img src=\"./../assets/images/cl_table_ref.png\" \n",
    "    alt=\"DevTool example for location of CL table element \" \n",
    "    style=\"width:1000px;height:600px;\"\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_results = driver.find_element_by_xpath('//*[@id=\"sortable-results\"]/ul')\n",
    "cl_results_items = cl_results.find_elements_by_tag_name(\"li\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://elpaso.craigslist.org/res/d/professional-talented-and-energetic/6966671942.html\n"
     ]
    }
   ],
   "source": [
    "item = cl_results_items[0]\n",
    "url = item.find_element_by_tag_name('a').get_attribute('href')\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom, we're able to pull URLs! We can now simple go through all the URLs we find and pull the information through selenium once again. \n",
    "\n",
    "Let's start with pulling resume information from the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we find the `XPath`, this time for the content body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = driver.find_element_by_xpath('//*[@id=\"postingbody\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reliable well organized professional with strong leadership qualities. Committed to producing results above and beyond what is expected. Strong in conflict resolutions. Your company will love to have an enthusiastic, discipline, knowledgeable & fast learner employee like me. Available IMMEDIATELY. Full-time job offer desirable.\n",
      "\n",
      "My career expertise areas are:\n",
      "• Customer Service\n",
      "• Customer Satisfaction\n",
      "• Salesperson\n",
      "• Administrative Assistant\n",
      "• Human Resources Assistant\n",
      "• Human Resources Specialist\n",
      "• Shift Supervisor\n",
      "• Management\n",
      "• Law Enforcement/Security Specialist\n",
      "\n",
      "I believe that I would be an assets to your company because my relevant knowledge, skills and abilities for this job includes:\n",
      "\n",
      "1. Bilingual Spanish & English\n",
      "2. Clerical Skills\n",
      "3. Microsoft Office Suite\n",
      "4. High Stress Environment\n",
      "5. High degree of initiative\n",
      "6. Proven Leadership\n",
      "7. Integrity\n",
      "8. Problem Solving\n",
      "9. High Degree of Initiative\n",
      "\n",
      "Over 15 years career in law enforcement with a successfully approved MBA in Human Resources.\n"
     ]
    }
   ],
   "source": [
    "print(content.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We are able to pull resume information. That's pretty much everything we need to start pulling data, we just need to summarize everything and run it as a loop. Let's create a function to do each step of the process. Ultimately, what we'd like to have is a single function that takes in the list of ({city}, {zipcode}) tuples, and spits out all the resumes found on CL. \n",
    "\n",
    "Let's break this down into the steps we performed above:\n",
    "    1. Going to the webpage via the city and zipcode (and distance).\n",
    "    2. Getting all the associate URLs for the location.\n",
    "    3. Going to the URLs and pulling the body contents.\n",
    "    4. Repeat step 1-3 until all 100 locations are scraped.\n",
    "    \n",
    "To keep the resume contents in a format that will make it easier to process later on, we're going to use a pandas dataframe, which is effectively a glorified Excel table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from ancillary import update_search_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CraigslistScrapper():\n",
    "    def __init__(self, driver, locations):\n",
    "        self.driver = driver\n",
    "        self.locations = locations\n",
    "        \n",
    "    # Function 1 to go to a particular webpage\n",
    "    def cl_resume_area(self, city, zipcode, distance=9999):\n",
    "        return self.driver.get(\"https://{}.craigslist.org/d/resumes/search/rrr?postal={}&search_distance={}\".format(city, zipcode, distance))\n",
    "    \n",
    "    # Function 2 to get all the associate URLs for the location.\n",
    "    def extract_urls(self):\n",
    "        urls = []\n",
    "        \n",
    "        # try statement to check if page exists\n",
    "        try: \n",
    "            n_pages = int(self.driver.find_element_by_xpath('//*[@id=\"searchform\"]/div[3]/div[3]/span[2]/span[3]/span[2]').text) // 120 + 1\n",
    "\n",
    "            for page in range(n_pages):\n",
    "                self.driver.get(update_search_page(self.driver.current_url, page))\n",
    "                cl_results = self.driver.find_element_by_xpath('//*[@id=\"sortable-results\"]/ul')\n",
    "                cl_results_items = cl_results.find_elements_by_tag_name(\"li\")\n",
    "                for item in cl_results_items:\n",
    "                    urls.append(item.find_element_by_tag_name('a').get_attribute('href'))\n",
    "\n",
    "            return urls\n",
    "        \n",
    "        except:\n",
    "            return urls\n",
    "        \n",
    "    # Function for 3 to get content\n",
    "    def extract_resume_content(self, url):\n",
    "        self.driver.get(url)\n",
    "\n",
    "        try:\n",
    "            return self.driver.find_element_by_xpath('//*[@id=\"postingbody\"]').text\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    # addtional function to determine if location is in a diff area i.e. NJ w/ NYC search\n",
    "    def extract_nearby_location(self):\n",
    "        return self.driver.find_element_by_xpath('//*[@id=\"sortable-results\"]/ul/li[1]/p/span[2]/span[1]').get_attribute('title')\n",
    "    \n",
    "    # Looping over all locations and saving as a dataframe\n",
    "    def execute_scrape(self):\n",
    "        self.raw_data = defaultdict(list)\n",
    "        \n",
    "        for city, zipcode in self.locations:\n",
    "            self.cl_resume_area(city, zipcode)\n",
    "            for url in self.extract_urls():\n",
    "                contents = self.extract_resume_content(url)\n",
    "                if contents is None:\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    city = self.extract_nearby_location()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                self.raw_data['city'].append(city)\n",
    "                self.raw_data['searched_zipcode'].append(zipcode)\n",
    "                self.raw_data['url'].append(url)\n",
    "                self.raw_data['content'].append(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = CraigslistScrapper(driver, webpage_table_info)  # GET ALL 100 WARNING, DO NOT RUN IN DEMO.\n",
    "# scraper = CraigslistScrapper(driver, [webpage_table_info[0]])\n",
    "scraper.execute_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(scraper.raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./../assets/data_large.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
